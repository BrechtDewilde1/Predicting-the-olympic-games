{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping the race times of:http://www.olympicgamesmarathon.com/results.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First obtain the HTML file of the main site. This site contains all the links to the actual datasets for each year Olympic Games where held."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url = \"http://www.olympicgamesmarathon.com/results.php\"\n",
    "site = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now BeautifulSoup will be used to parse the HTML structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(site.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# link.attrs can be used to obtain a dictionary with the key and value of alll the attributes a tag has\n",
    "for link in soup.find_all(\"a\")[:20]:\n",
    "    print(link.attrs['href'].find(\"olympiad\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see from this that we only want these links whereby the href contains olympiad in it. However, some links occur multiple times. So probably best to first select all the olympiad names. And then make a unique list from this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "links = []\n",
    "# obtain all olympiad.php's\n",
    "# find method returns -1 in case the substring is not found\n",
    "for link in soup.find_all(\"a\"):\n",
    "    if link.attrs[\"href\"].find(\"olympiad\") >= 0:\n",
    "        links.append(link.attrs[\"href\"])\n",
    "\n",
    "# make the list unique\n",
    "links = set(links)\n",
    "\n",
    "# add 'http://www.olympicgamesmarathon.com/' in front of each element of the list \n",
    "full_links = []\n",
    "prefix = 'http://www.olympicgamesmarathon.com/'\n",
    "for link in links:\n",
    "    full_links.append(prefix + link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is no to obtain the HTML structure of each of these links. Then to parse these html structures to finally obtain the data stored in each of these pages. This has to happen all inside a for loop. I thinks it will be best to write the code for one year. Once this code is fullfilled then this code will be generic for all the years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "link = full_links[0]\n",
    "page = requests.get(link)\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a specific table containing information about the people that did not finish. As I don't know the target value for these athletes it is useless to store these atheletes. Therefore, i will only create a table containing information about the athelest whereof i have their race time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for link in full_links:\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "\n",
    "    # Assign dfName to variable\n",
    "    dfName = 'Runtimes_' +  re.search(\"\\d+\", link).group() + '.csv'\n",
    "\n",
    "    # find the indices where the table containing the racers starst and ends\n",
    "    start = 0\n",
    "    end = 0\n",
    "    for index, value in enumerate(soup.find_all(\"td\")):\n",
    "        if value.text == \"1Â°\":\n",
    "            start = index\n",
    "        if value.text == \"Did not finish\":\n",
    "            end = index - 1\n",
    "            break\n",
    "\n",
    "    # Creating an empty Dataframe with column names only\n",
    "    dfObj = pd.DataFrame(columns=['Position', 'FirstName', \"LastName\",\"Nationality\", \"Time\"])\n",
    "\n",
    "    # each innerlist is one specific racer with it results \n",
    "    racers = []\n",
    "    l = 0\n",
    "    for i in range(5, end + 4, 5):\n",
    "        racers.append(soup.find_all(\"td\")[start:end][l:i])\n",
    "        l += 5\n",
    "\n",
    "    # converting attributes to text\n",
    "    for racer in racers:\n",
    "        if len(racer) == 0:\n",
    "            break\n",
    "        else:\n",
    "            for i in range(5):\n",
    "                if i == 0:\n",
    "                    racer[i] = racer[i].text[:-1]\n",
    "                else:\n",
    "                    racer[i] = racer[i].text\n",
    "\n",
    "    # filling in the dataframe\n",
    "    for racer in racers:\n",
    "        if len(racer) == 0:\n",
    "            break\n",
    "        else:\n",
    "            dfObj = dfObj.append({'Position':racer[0] , 'FirstName':racer[1], 'LastName':racer[2], 'Nationality':racer[3],\n",
    "                              'Time':racer[4]}, ignore_index=True)\n",
    "    \n",
    "    # write the pandas dataframe to csv file with correct name \n",
    "    dfObj.to_csv(r'C:\\Users\\BrechtDewilde\\Documents\\UGENT -  statistical data analysis\\STATISTICAL DATA ANALYSIS\\thesis\\R analysis\\Data\\scraped\\race times\\ ' + dfName, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
